# 实现效果
（1）浏览器地址栏输入地址 http://192.168.17.129/edu/a.html 负载均衡效果，平均到 8080和8081 端口中
# 准备工作

（1）准备两台 tomcat 服务器，一台 8080，一台 8081

（2）在两台 tomcat 里面 webapps 目录中，创建名称是 edu 文件夹，在 edu 文件夹中创建页面 a.html，用于测试（类似反向代理2的配置)
# 在 nginx 的配置文件中进行负载均衡的配置
在`nginx.conf`中添加：
```
upstream myserver{
        server 192.168.77.130:8080;
        server 192.168.77.130:8081;
}
```
![[Pasted image 20230731162723.png]]

```C
proxy_pass http://myserver;
```

![[Pasted image 20230731162939.png]]

重启nginx
./nginx -s reload
# 测试
http://192.168.77.130/edu/a.html
访问结果在 8081 和 8080 之间切换 。
![[Pasted image 20230731163042.png]]

![[Pasted image 20230731163050.png]]

# 负载均衡策略
随着互联网信息的爆炸性增长，负载均衡（load balance）已经不再是一个很陌生的话题，顾名思义，负载均衡即是将负载分摊到不同的服务单元，既保证服务的可用性，又保证响应足够快，给用户很好的体验。快速增长的访问量和数据流量催生了各式各样的负载均衡产品，很多专业的负载均衡硬件提供了很好的功能，但却价格不菲，这使得负载均衡软件大受欢迎， nginx 就是其中的一个，在 linux 下有 Nginx、LVS、Haproxy 等等服务可以提供负载均衡服务，而且 Nginx 提供了几种分配方式(策略)：

### 轮询（默认）
个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。
```C
upstream myserver{
        server 192.168.77.130:8080;
        server 192.168.77.130:8081;
}

```
## weight
weight 代表权,重默认为 1,权重越高被分配的客户端越多 。

指定轮询几率，weight 和访问比率成正比，用于后端服务器性能不均的情况。 例如：
```C
upstream myserver{ 
server 192.168.77.130:8080 weight=10; 
server 192.168.77.130:8081 weight=5; 
}
```

## ip_hash
每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题
==也就是说客户端在第一次访问某个服务后，后面都是访问此服务器 。==
如果客户端换了，那么提供服务的服务器也会换。
```C
upstream myserver{ ip_hash; 
						server 192.168.77.130:8080; 
						server 192.168.77.130:8081; }
```

## fair（第三方）
按后端服务器的响应时间来分配请求，响应时间短的优先分配。
```C
upstream myserver{
             server 192.168.77.130:8080;
             server 192.168.77.130:8081;
             fair;
}
```

